{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Dark Knowledge in MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to implement Bayesian Dark Knowledge (Korattikara, Rathod, Murphy and Welling, 2015) in MXNet.\n",
    "In applications like recommendation and control, bayesian treatment of neural networks may be helpful in that we can model the uncertainty of our prediction to avoid overconfident actions (Yeung, Hao and Naiyan, 2015). However, bayesian parameter estimation is non-trivial and much more difficult than a simple point estimation due to the high-dimensionality and non-linearity of neural networks. One way to tackle the problem is the expectation propagation approach in (Hern√°ndez-Lobato and Adams, 2015), which relies on a predefined parameteric form of the posterior distribution. The Bayesian Dark Knowledge (BDK) implemented in this notebook is another solution that uses Stochastic Gradient Langevin Dynamics (SGLD) to draw samples from the posterior of the bayesian neural network and fit a student network use these teaching samples. BDK can achieve similar performance as the SGLD teacher while being much faster for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import numpy\n",
    "import time\n",
    "import ssl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(training_num=50000):\n",
    "    data_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), 'mnist.npz')\n",
    "    if not os.path.isfile(data_path):\n",
    "        from six.moves import urllib\n",
    "        origin = (\n",
    "            'https://github.com/sxjscience/mxnet/raw/master/example/bayesian-methods/mnist.npz'\n",
    "        )\n",
    "        print ('Downloading data from {} to {}'.format(origin, data_path))\n",
    "        urllib.request.urlretrieve(origin, data_path)\n",
    "        print ('Done!')\n",
    "    dat = numpy.load(data_path)\n",
    "    X = (dat['X'][:training_num] / 126.0).astype('float32')\n",
    "    Y = dat['Y'][:training_num]\n",
    "    X_test = (dat['X_test'] / 126.0).astype('float32')\n",
    "    Y_test = dat['Y_test']\n",
    "    Y = Y.reshape((Y.shape[0],))\n",
    "    Y_test = Y_test.reshape((Y_test.shape[0],))\n",
    "    return X, Y, X_test, Y_test\n",
    "\n",
    "\n",
    "def sample_test_acc(exe, X, Y, label_num=None, minibatch_size=100):\n",
    "    pred = numpy.zeros((X.shape[0], label_num)).astype('float32')\n",
    "    iter = mx.io.NDArrayIter(data=X, label=Y, batch_size=minibatch_size, shuffle=False)\n",
    "    curr_instance = 0\n",
    "    iter.reset()\n",
    "    for batch in iter:\n",
    "        exe.arg_dict['data'][:] = batch.data[0]\n",
    "        exe.forward(is_train=False)\n",
    "        batch_size = minibatch_size - batch.pad\n",
    "        pred[curr_instance:curr_instance + minibatch_size - batch.pad, :] += exe.outputs[0].asnumpy()[:batch_size]\n",
    "        curr_instance += batch_size\n",
    "    correct = (pred.argmax(axis=1) == Y).sum()\n",
    "    total = Y.shape[0]\n",
    "    acc = correct/float(total)\n",
    "    return correct, total, acc\n",
    "\n",
    "\n",
    "def get_executor(sym, ctx, data_inputs, initializer=None):\n",
    "    data_shapes = {k: v.shape for k, v in data_inputs.items()}\n",
    "    arg_names = sym.list_arguments()\n",
    "    aux_names = sym.list_auxiliary_states()\n",
    "    param_names = list(set(arg_names) - set(data_inputs.keys()))\n",
    "    arg_shapes, output_shapes, aux_shapes = sym.infer_shape(**data_shapes)\n",
    "    arg_name_shape = {k: s for k, s in zip(arg_names, arg_shapes)}\n",
    "    params = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n",
    "    params_grad = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n",
    "    aux_states = {k: nd.empty(s, ctx=ctx) for k, s in zip(aux_names, aux_shapes)}\n",
    "    exe = sym.bind(ctx=ctx, args=dict(params, **data_inputs),\n",
    "                   args_grad=params_grad,\n",
    "                   aux_states=aux_states)\n",
    "    if initializer != None:\n",
    "        for k, v in params.items():\n",
    "            initializer(k, v)\n",
    "    return exe, params, params_grad, aux_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistilledSGLD(teacher_sym, student_sym,\n",
    "                  teacher_data_inputs, student_data_inputs,\n",
    "                  X, Y, X_test, Y_test, total_iter_num,\n",
    "                  teacher_learning_rate, student_learning_rate,\n",
    "                  teacher_lr_scheduler=None, student_lr_scheduler=None,\n",
    "                  student_optimizing_algorithm='adam',\n",
    "                  teacher_prior_precision=1, student_prior_precision=0.001,\n",
    "                  perturb_deviation=0.001,\n",
    "                  student_initializer=None,\n",
    "                  teacher_initializer=None,\n",
    "                  minibatch_size=100,\n",
    "                  dev=mx.gpu()):\n",
    "    teacher_exe, teacher_params, teacher_params_grad, _ = get_executor(teacher_sym, dev, teacher_data_inputs, teacher_initializer)\n",
    "    student_exe, student_params, student_params_grad, _ = get_executor(student_sym, dev, student_data_inputs, student_initializer)\n",
    "    teacher_label_key = list(set(teacher_data_inputs.keys()) - set(['data']))[0]\n",
    "    student_label_key = list(set(student_data_inputs.keys()) - set(['data']))[0]\n",
    "    teacher_optimizer = mx.optimizer.create('sgld',\n",
    "                                            learning_rate=teacher_learning_rate,\n",
    "                                            rescale_grad=X.shape[0] / float(minibatch_size),\n",
    "                                            lr_scheduler=teacher_lr_scheduler,\n",
    "                                            wd=teacher_prior_precision)\n",
    "    student_optimizer = mx.optimizer.create(student_optimizing_algorithm,\n",
    "                                            learning_rate=student_learning_rate,\n",
    "                                            rescale_grad=1.0 / float(minibatch_size),\n",
    "                                            lr_scheduler=student_lr_scheduler,\n",
    "                                            wd=student_prior_precision)\n",
    "    teacher_updater = mx.optimizer.get_updater(teacher_optimizer)\n",
    "    student_updater = mx.optimizer.get_updater(student_optimizer)\n",
    "    start = time.time()\n",
    "    for i in range(total_iter_num):\n",
    "        # 1.1 Draw random minibatch\n",
    "        indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n",
    "        X_batch = X[indices]\n",
    "        Y_batch = Y[indices]\n",
    "        \n",
    "        # 1.2 Update teacher\n",
    "        teacher_exe.arg_dict['data'][:] = X_batch\n",
    "        teacher_exe.arg_dict[teacher_label_key][:] = Y_batch\n",
    "        teacher_exe.forward(is_train=True)\n",
    "        teacher_exe.backward()       \n",
    "        for k in teacher_params:\n",
    "            teacher_updater(k, teacher_params_grad[k], teacher_params[k])\n",
    "    \n",
    "        # 2.1 Draw random minibatch and do random perturbation\n",
    "        indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n",
    "        X_student_batch = X[indices] + numpy.random.normal(0, perturb_deviation, X_batch.shape).astype('float32')\n",
    "\n",
    "        # 2.2 Get teacher predictions\n",
    "        teacher_exe.arg_dict['data'][:] = X_student_batch\n",
    "        teacher_exe.forward(is_train=False)\n",
    "        teacher_pred = teacher_exe.outputs[0]\n",
    "        teacher_pred.wait_to_read()\n",
    "\n",
    "        # 2.3 Update student\n",
    "        student_exe.arg_dict['data'][:] = X_student_batch\n",
    "        student_exe.arg_dict[student_label_key][:] = teacher_pred\n",
    "        student_exe.forward(is_train=True)\n",
    "        student_exe.backward()\n",
    "        for k in student_params:\n",
    "            student_updater(k, student_params_grad[k], student_params[k])\n",
    "\n",
    "        if (i + 1) % 2000 == 0:\n",
    "            end = time.time()\n",
    "            print (\"Current Iter Num: {:d}\".format(i + 1) + \"Time Spent: {:f}\".format(end - start))\n",
    "            test_correct, test_total, test_acc = \\\n",
    "                sample_test_acc(student_exe, X=X_test, Y=Y_test, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            train_correct, train_total, train_acc = \\\n",
    "                sample_test_acc(student_exe, X=X, Y=Y, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            teacher_test_correct, teacher_test_total, teacher_test_acc = \\\n",
    "                sample_test_acc(teacher_exe, X=X_test, Y=Y_test, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            teacher_train_correct, teacher_train_total, teacher_train_acc = \\\n",
    "                sample_test_acc(teacher_exe, X=X, Y=Y, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            print (\"Student: Test {:d}/{:d}={:f}, Train {:d}/{:d}={:f}\".format(test_correct, test_total, test_acc,\n",
    "                                                       train_correct, train_total, train_acc))\n",
    "            print (\"Teacher: Test {:d}/{:d}={:f}, Train {:d}/{:d}={:f}\".format(teacher_test_correct, teacher_test_total, teacher_test_acc,\n",
    "                                                    teacher_train_correct, teacher_train_total, teacher_train_acc))\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropySoftmax(mx.operator.NumpyOp):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropySoftmax, self).__init__(False)\n",
    "\n",
    "    def list_arguments(self):\n",
    "        return ['data', 'label']\n",
    "\n",
    "    def list_outputs(self):\n",
    "        return ['output']\n",
    "\n",
    "    def infer_shape(self, in_shape):\n",
    "        data_shape = in_shape[0]\n",
    "        label_shape = in_shape[0]\n",
    "        output_shape = in_shape[0]\n",
    "        return [data_shape, label_shape], [output_shape]\n",
    "\n",
    "    def forward(self, in_data, out_data):\n",
    "        x = in_data[0]\n",
    "        y = out_data[0]\n",
    "        y[:] = numpy.exp(x - x.max(axis=1).reshape((x.shape[0], 1))).astype('float32')\n",
    "        y /= y.sum(axis=1).reshape((x.shape[0], 1))\n",
    "\n",
    "    def backward(self, out_grad, in_data, out_data, in_grad):\n",
    "        l = in_data[1]\n",
    "        y = out_data[0]\n",
    "        dx = in_grad[0]\n",
    "        dx[:] = (y - l)\n",
    "\n",
    "        \n",
    "class BiasXavier(mx.initializer.Xavier):\n",
    "    def _init_bias(self, _, arr):\n",
    "        scale = numpy.sqrt(self.magnitude / arr.shape[0])\n",
    "        mx.random.uniform(-scale, scale, out=arr)\n",
    "        \n",
    "        \n",
    "def get_mnist_sym(output_op=None, num_hidden=400):\n",
    "    net = mx.symbol.Variable('data')\n",
    "    net = mx.symbol.FullyConnected(data=net, name='mnist_fc1', num_hidden=num_hidden)\n",
    "    net = mx.symbol.Activation(data=net, name='mnist_relu1', act_type=\"relu\")\n",
    "    net = mx.symbol.FullyConnected(data=net, name='mnist_fc2', num_hidden=num_hidden)\n",
    "    net = mx.symbol.Activation(data=net, name='mnist_relu2', act_type=\"relu\")\n",
    "    net = mx.symbol.FullyConnected(data=net, name='mnist_fc3', num_hidden=10)\n",
    "    if output_op is None:\n",
    "        net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n",
    "    else:\n",
    "        net = output_op(data=net, name='softmax')\n",
    "    return net\n",
    "\n",
    "def dev():\n",
    "    return mx.gpu()\n",
    "\n",
    "def run_mnist_DistilledSGLD(training_num=50000):\n",
    "    X, Y, X_test, Y_test = load_mnist(training_num)\n",
    "    minibatch_size = 100\n",
    "    \n",
    "    # Paper's settings\n",
    "    num_hidden = 50\n",
    "    total_iter_num = 20000\n",
    "    teacher_learning_rate = 4E-6\n",
    "    student_learning_rate = 0.005\n",
    "    teacher_prior = 1\n",
    "    student_prior = 0.001\n",
    "    perturb_deviation = 0.001 # standard deviation of Gaussian noise\n",
    "\n",
    "# original settings\n",
    "#     if training_num >= 10000:\n",
    "#         num_hidden = 800\n",
    "#         total_iter_num = 1000000\n",
    "#         teacher_learning_rate = 1E-6\n",
    "#         student_learning_rate = 0.0001\n",
    "#         teacher_prior = 1\n",
    "#         student_prior = 0.1\n",
    "#         perturb_deviation = 0.1\n",
    "#     else:\n",
    "#         num_hidden = 400\n",
    "#         total_iter_num = 20000\n",
    "#         teacher_learning_rate = 4E-5\n",
    "#         student_learning_rate = 0.0001\n",
    "#         teacher_prior = 1\n",
    "#         student_prior = 0.1\n",
    "#         perturb_deviation = 0.001\n",
    "        \n",
    "    teacher_net = get_mnist_sym(num_hidden=num_hidden)\n",
    "    crossentropy_softmax = CrossEntropySoftmax()\n",
    "    student_net = get_mnist_sym(output_op=crossentropy_softmax, num_hidden=num_hidden)\n",
    "    data_shape = (minibatch_size,) + X.shape[1::]\n",
    "    teacher_data_inputs = {'data': nd.zeros(data_shape, ctx=dev()),\n",
    "                           'softmax_label': nd.zeros((minibatch_size,), ctx=dev())}\n",
    "    student_data_inputs = {'data': nd.zeros(data_shape, ctx=dev()),\n",
    "                           'softmax_label': nd.zeros((minibatch_size, 10), ctx=dev())}\n",
    "    teacher_initializer = BiasXavier(factor_type=\"in\", magnitude=1)\n",
    "    student_initializer = BiasXavier(factor_type=\"in\", magnitude=1)\n",
    "    DistilledSGLD(teacher_sym=teacher_net, student_sym=student_net,\n",
    "                  teacher_data_inputs=teacher_data_inputs,\n",
    "                  student_data_inputs=student_data_inputs,\n",
    "                  X=X, Y=Y, X_test=X_test, Y_test=Y_test, total_iter_num=total_iter_num,\n",
    "                  student_initializer=student_initializer,\n",
    "                  teacher_initializer=teacher_initializer,\n",
    "                  student_optimizing_algorithm=\"adam\",\n",
    "                  teacher_learning_rate=teacher_learning_rate,\n",
    "                  student_learning_rate=student_learning_rate,\n",
    "                  teacher_prior_precision=teacher_prior, student_prior_precision=student_prior,\n",
    "                  perturb_deviation=perturb_deviation, minibatch_size=100, dev=dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/ipykernel_launcher.py:53: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter Num: 2000Time Spent: 13.310685\n",
      "Student: Test 9410/10000=0.941000, Train 47071/50000=0.941420\n",
      "Teacher: Test 9494/10000=0.949400, Train 47755/50000=0.955100\n",
      "Current Iter Num: 4000Time Spent: 12.779635\n",
      "Student: Test 9437/10000=0.943700, Train 47357/50000=0.947140\n",
      "Teacher: Test 9533/10000=0.953300, Train 48229/50000=0.964580\n",
      "Current Iter Num: 6000Time Spent: 12.673412\n",
      "Student: Test 9516/10000=0.951600, Train 47767/50000=0.955340\n",
      "Teacher: Test 9572/10000=0.957200, Train 48535/50000=0.970700\n",
      "Current Iter Num: 8000Time Spent: 13.051386\n",
      "Student: Test 9521/10000=0.952100, Train 47639/50000=0.952780\n",
      "Teacher: Test 9622/10000=0.962200, Train 48832/50000=0.976640\n",
      "Current Iter Num: 10000Time Spent: 12.597862\n",
      "Student: Test 9527/10000=0.952700, Train 47923/50000=0.958460\n",
      "Teacher: Test 9615/10000=0.961500, Train 48878/50000=0.977560\n",
      "Current Iter Num: 12000Time Spent: 13.045359\n",
      "Student: Test 9534/10000=0.953400, Train 47798/50000=0.955960\n",
      "Teacher: Test 9599/10000=0.959900, Train 48966/50000=0.979320\n",
      "Current Iter Num: 14000Time Spent: 12.636362\n",
      "Student: Test 9505/10000=0.950500, Train 47829/50000=0.956580\n",
      "Teacher: Test 9586/10000=0.958600, Train 48990/50000=0.979800\n",
      "Current Iter Num: 16000Time Spent: 12.987522\n",
      "Student: Test 9602/10000=0.960200, Train 48125/50000=0.962500\n",
      "Teacher: Test 9592/10000=0.959200, Train 48989/50000=0.979780\n",
      "Current Iter Num: 18000Time Spent: 13.515383\n",
      "Student: Test 9562/10000=0.956200, Train 48106/50000=0.962120\n",
      "Teacher: Test 9636/10000=0.963600, Train 49087/50000=0.981740\n",
      "Current Iter Num: 20000Time Spent: 13.365021\n",
      "Student: Test 9556/10000=0.955600, Train 48108/50000=0.962160\n",
      "Teacher: Test 9625/10000=0.962500, Train 49162/50000=0.983240\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(100)\n",
    "mx.random.seed(100)\n",
    "run_mnist_DistilledSGLD(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
