{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Dark Knowledge in MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to implement Bayesian Dark Knowledge (Korattikara, Rathod, Murphy and Welling, 2015) in MXNet.\n",
    "In applications like recommendation and control, bayesian treatment of neural networks may be helpful in that we can model the uncertainty of our prediction to avoid overconfident actions (Yeung, Hao and Naiyan, 2015). However, bayesian parameter estimation is non-trivial and much more difficult than a simple point estimation due to the high-dimensionality and non-linearity of neural networks. One way to tackle the problem is the expectation propagation approach in (HernÃ¡ndez-Lobato and Adams, 2015), which relies on a predefined parameteric form of the posterior distribution. The Bayesian Dark Knowledge (BDK) implemented in this notebook is another solution that uses Stochastic Gradient Langevin Dynamics (SGLD) to draw samples from the posterior of the bayesian neural network and fit a student network use these teaching samples. BDK can achieve similar performance as the SGLD teacher while being much faster for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import numpy\n",
    "import time\n",
    "import ssl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(training_num=50000):\n",
    "    data_path = os.path.join(os.path.dirname(os.path.realpath('__file__')), 'mnist.npz')\n",
    "    if not os.path.isfile(data_path):\n",
    "        from six.moves import urllib\n",
    "        origin = (\n",
    "            'https://github.com/sxjscience/mxnet/raw/master/example/bayesian-methods/mnist.npz'\n",
    "        )\n",
    "        print ('Downloading data from {} to {}'.format(origin, data_path))\n",
    "        #context = ssl._create_unverified_context()\n",
    "        urllib.request.urlretrieve(origin, data_path)\n",
    "        print ('Done!')\n",
    "    dat = numpy.load(data_path)\n",
    "    X = (dat['X'][:training_num] / 126.0).astype('float32')\n",
    "    Y = dat['Y'][:training_num]\n",
    "    X_test = (dat['X_test'] / 126.0).astype('float32')\n",
    "    Y_test = dat['Y_test']\n",
    "    Y = Y.reshape((Y.shape[0],))\n",
    "    Y_test = Y_test.reshape((Y_test.shape[0],))\n",
    "    return X, Y, X_test, Y_test\n",
    "\n",
    "\n",
    "def sample_test_acc(exe, X, Y, label_num=None, minibatch_size=100):\n",
    "    pred = numpy.zeros((X.shape[0], label_num)).astype('float32')\n",
    "    iter = mx.io.NDArrayIter(data=X, label=Y, batch_size=minibatch_size, shuffle=False)\n",
    "    curr_instance = 0\n",
    "    iter.reset()\n",
    "    for batch in iter:\n",
    "        exe.arg_dict['data'][:] = batch.data[0]\n",
    "        exe.forward(is_train=False)\n",
    "        batch_size = minibatch_size - batch.pad\n",
    "        pred[curr_instance:curr_instance + minibatch_size - batch.pad, :] += exe.outputs[0].asnumpy()[:batch_size]\n",
    "        curr_instance += batch_size\n",
    "    correct = (pred.argmax(axis=1) == Y).sum()\n",
    "    total = Y.shape[0]\n",
    "    acc = correct/float(total)\n",
    "    return correct, total, acc\n",
    "\n",
    "\n",
    "def get_executor(sym, ctx, data_inputs, initializer=None):\n",
    "    data_shapes = {k: v.shape for k, v in data_inputs.items()}\n",
    "    arg_names = sym.list_arguments()\n",
    "    aux_names = sym.list_auxiliary_states()\n",
    "    param_names = list(set(arg_names) - set(data_inputs.keys()))\n",
    "    arg_shapes, output_shapes, aux_shapes = sym.infer_shape(**data_shapes)\n",
    "    arg_name_shape = {k: s for k, s in zip(arg_names, arg_shapes)}\n",
    "    params = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n",
    "    params_grad = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n",
    "    aux_states = {k: nd.empty(s, ctx=ctx) for k, s in zip(aux_names, aux_shapes)}\n",
    "    exe = sym.bind(ctx=ctx, args=dict(params, **data_inputs),\n",
    "                   args_grad=params_grad,\n",
    "                   aux_states=aux_states)\n",
    "    if initializer != None:\n",
    "        for k, v in params.items():\n",
    "            initializer(k, v)\n",
    "    return exe, params, params_grad, aux_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistilledSGLD(teacher_sym, student_sym,\n",
    "                  teacher_data_inputs, student_data_inputs,\n",
    "                  X, Y, X_test, Y_test, total_iter_num,\n",
    "                  teacher_learning_rate, student_learning_rate,\n",
    "                  teacher_lr_scheduler=None, student_lr_scheduler=None,\n",
    "                  student_optimizing_algorithm='adam',\n",
    "                  teacher_prior_precision=1, student_prior_precision=0.001,\n",
    "                  perturb_deviation=0.001,\n",
    "                  student_initializer=None,\n",
    "                  teacher_initializer=None,\n",
    "                  minibatch_size=100,\n",
    "                  dev=mx.gpu()):\n",
    "    teacher_exe, teacher_params, teacher_params_grad, _ = \\\n",
    "        get_executor(teacher_sym, dev, teacher_data_inputs, teacher_initializer)\n",
    "    student_exe, student_params, student_params_grad, _ = \\\n",
    "        get_executor(student_sym, dev, student_data_inputs, student_initializer)\n",
    "    teacher_label_key = list(set(teacher_data_inputs.keys()) - set(['data']))[0]\n",
    "    student_label_key = list(set(student_data_inputs.keys()) - set(['data']))[0]\n",
    "    teacher_optimizer = mx.optimizer.create('sgld',\n",
    "                                            learning_rate=teacher_learning_rate,\n",
    "                                            rescale_grad=X.shape[0] / float(minibatch_size),\n",
    "                                            lr_scheduler=teacher_lr_scheduler,\n",
    "                                            wd=teacher_prior_precision)\n",
    "    student_optimizer = mx.optimizer.create(student_optimizing_algorithm,\n",
    "                                            learning_rate=student_learning_rate,\n",
    "                                            rescale_grad=1.0 / float(minibatch_size),\n",
    "                                            lr_scheduler=student_lr_scheduler,\n",
    "                                            wd=student_prior_precision)\n",
    "    teacher_updater = mx.optimizer.get_updater(teacher_optimizer)\n",
    "    student_updater = mx.optimizer.get_updater(student_optimizer)\n",
    "    start = time.time()\n",
    "    for i in range(total_iter_num):\n",
    "        # 1.1 Draw random minibatch\n",
    "        indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n",
    "        X_batch = X[indices]\n",
    "        Y_batch = Y[indices]\n",
    "        \n",
    "        # 1.2 Update teacher\n",
    "        teacher_exe.arg_dict['data'][:] = X_batch\n",
    "        teacher_exe.arg_dict[teacher_label_key][:] = Y_batch\n",
    "        teacher_exe.forward(is_train=True)\n",
    "        teacher_exe.backward()       \n",
    "        for k in teacher_params:\n",
    "            teacher_updater(k, teacher_params_grad[k], teacher_params[k])\n",
    "    \n",
    "        # 2.1 Draw random minibatch and do random perturbation\n",
    "        indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n",
    "        X_student_batch = X[indices] + numpy.random.normal(0, perturb_deviation, X_batch.shape).astype('float32')\n",
    "\n",
    "        # 2.2 Get teacher predictions\n",
    "        teacher_exe.arg_dict['data'][:] = X_student_batch\n",
    "        teacher_exe.forward(is_train=False)\n",
    "        teacher_pred = teacher_exe.outputs[0]\n",
    "        teacher_pred.wait_to_read()\n",
    "\n",
    "        # 2.3 Update student\n",
    "        student_exe.arg_dict['data'][:] = X_student_batch\n",
    "        student_exe.arg_dict[student_label_key][:] = teacher_pred\n",
    "        student_exe.forward(is_train=True)\n",
    "        student_exe.backward()\n",
    "        for k in student_params:\n",
    "            student_updater(k, student_params_grad[k], student_params[k])\n",
    "\n",
    "        if (i + 1) % 2000 == 0:\n",
    "            end = time.time()\n",
    "            print (\"Current Iter Num: {:d}\".format(i + 1) + \"Time Spent: {:f}\".format(end - start))\n",
    "            test_correct, test_total, test_acc = \\\n",
    "                sample_test_acc(student_exe, X=X_test, Y=Y_test, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            train_correct, train_total, train_acc = \\\n",
    "                sample_test_acc(student_exe, X=X, Y=Y, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            teacher_test_correct, teacher_test_total, teacher_test_acc = \\\n",
    "                sample_test_acc(teacher_exe, X=X_test, Y=Y_test, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            teacher_train_correct, teacher_train_total, teacher_train_acc = \\\n",
    "                sample_test_acc(teacher_exe, X=X, Y=Y, label_num=10,\n",
    "                                minibatch_size=minibatch_size)\n",
    "            print (\"Student: Test {:d}/{:d}={:f}, Train {:d}/{:d}={:f}\".format(test_correct, test_total, test_acc,\n",
    "                                                       train_correct, train_total, train_acc))\n",
    "            print (\"Teacher: Test {:d}/{:d}={:f}, Train {:d}/{:d}={:f}\".format(teacher_test_correct, teacher_test_total, teacher_test_acc,\n",
    "                                                    teacher_train_correct, teacher_train_total, teacher_train_acc))\n",
    "            start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropySoftmax(mx.operator.NumpyOp):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropySoftmax, self).__init__(False)\n",
    "\n",
    "    def list_arguments(self):\n",
    "        return ['data', 'label']\n",
    "\n",
    "    def list_outputs(self):\n",
    "        return ['output']\n",
    "\n",
    "    def infer_shape(self, in_shape):\n",
    "        data_shape = in_shape[0]\n",
    "        label_shape = in_shape[0]\n",
    "        output_shape = in_shape[0]\n",
    "        return [data_shape, label_shape], [output_shape]\n",
    "\n",
    "    def forward(self, in_data, out_data):\n",
    "        x = in_data[0]\n",
    "        y = out_data[0]\n",
    "        y[:] = numpy.exp(x - x.max(axis=1).reshape((x.shape[0], 1))).astype('float32')\n",
    "        y /= y.sum(axis=1).reshape((x.shape[0], 1))\n",
    "\n",
    "    def backward(self, out_grad, in_data, out_data, in_grad):\n",
    "        l = in_data[1]\n",
    "        y = out_data[0]\n",
    "        dx = in_grad[0]\n",
    "        dx[:] = (y - l)\n",
    "\n",
    "        \n",
    "class BiasXavier(mx.initializer.Xavier):\n",
    "    def _init_bias(self, _, arr):\n",
    "        scale = numpy.sqrt(self.magnitude / arr.shape[0])\n",
    "        mx.random.uniform(-scale, scale, out=arr)\n",
    "        \n",
    "        \n",
    "def get_mnist_sym(output_op=None, num_hidden=400):\n",
    "    net = mx.symbol.Variable('data')\n",
    "    net = mx.symbol.FullyConnected(data=net, name='mnist_fc1', num_hidden=num_hidden)\n",
    "    net = mx.symbol.Activation(data=net, name='mnist_relu1', act_type=\"relu\")\n",
    "    net = mx.symbol.FullyConnected(data=net, name='mnist_fc2', num_hidden=num_hidden)\n",
    "    net = mx.symbol.Activation(data=net, name='mnist_relu2', act_type=\"relu\")\n",
    "    net = mx.symbol.FullyConnected(data=net, name='mnist_fc3', num_hidden=10)\n",
    "    if output_op is None:\n",
    "        net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n",
    "    else:\n",
    "        net = output_op(data=net, name='softmax')\n",
    "    return net\n",
    "\n",
    "def dev():\n",
    "    return mx.gpu()\n",
    "\n",
    "def run_mnist_DistilledSGLD(training_num=50000):\n",
    "    X, Y, X_test, Y_test = load_mnist(training_num)\n",
    "    minibatch_size = 100\n",
    "    if training_num >= 10000:\n",
    "        num_hidden = 800\n",
    "        total_iter_num = 1000000\n",
    "        teacher_learning_rate = 1E-6\n",
    "        student_learning_rate = 0.0001\n",
    "        teacher_prior = 1\n",
    "        student_prior = 0.1\n",
    "        perturb_deviation = 0.1\n",
    "    else:\n",
    "        num_hidden = 400\n",
    "        total_iter_num = 20000\n",
    "        teacher_learning_rate = 4E-5\n",
    "        student_learning_rate = 0.0001\n",
    "        teacher_prior = 1\n",
    "        student_prior = 0.1\n",
    "        perturb_deviation = 0.001\n",
    "    teacher_net = get_mnist_sym(num_hidden=num_hidden)\n",
    "    crossentropy_softmax = CrossEntropySoftmax()\n",
    "    student_net = get_mnist_sym(output_op=crossentropy_softmax, num_hidden=num_hidden)\n",
    "    data_shape = (minibatch_size,) + X.shape[1::]\n",
    "    teacher_data_inputs = {'data': nd.zeros(data_shape, ctx=dev()),\n",
    "                           'softmax_label': nd.zeros((minibatch_size,), ctx=dev())}\n",
    "    student_data_inputs = {'data': nd.zeros(data_shape, ctx=dev()),\n",
    "                           'softmax_label': nd.zeros((minibatch_size, 10), ctx=dev())}\n",
    "    teacher_initializer = BiasXavier(factor_type=\"in\", magnitude=1)\n",
    "    student_initializer = BiasXavier(factor_type=\"in\", magnitude=1)\n",
    "    DistilledSGLD(teacher_sym=teacher_net, student_sym=student_net,\n",
    "                  teacher_data_inputs=teacher_data_inputs,\n",
    "                  student_data_inputs=student_data_inputs,\n",
    "                  X=X, Y=Y, X_test=X_test, Y_test=Y_test, total_iter_num=total_iter_num,\n",
    "                  student_initializer=student_initializer,\n",
    "                  teacher_initializer=teacher_initializer,\n",
    "                  student_optimizing_algorithm=\"adam\",\n",
    "                  teacher_learning_rate=teacher_learning_rate,\n",
    "                  student_learning_rate=student_learning_rate,\n",
    "                  teacher_prior_precision=teacher_prior, student_prior_precision=student_prior,\n",
    "                  perturb_deviation=perturb_deviation, minibatch_size=100, dev=dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py35/lib/python3.5/site-packages/ipykernel_launcher.py:54: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Iter Num: 2000Time Spent: 13.546626\n",
      "Student: Test 7991/10000=0.799100, Train 459/500=0.918000\n",
      "Teacher: Test 7191/10000=0.719100, Train 494/500=0.988000\n",
      "Current Iter Num: 4000Time Spent: 13.525249\n",
      "Student: Test 8014/10000=0.801400, Train 464/500=0.928000\n",
      "Teacher: Test 7259/10000=0.725900, Train 499/500=0.998000\n",
      "Current Iter Num: 6000Time Spent: 13.126004\n",
      "Student: Test 7955/10000=0.795500, Train 466/500=0.932000\n",
      "Teacher: Test 7430/10000=0.743000, Train 495/500=0.990000\n",
      "Current Iter Num: 8000Time Spent: 13.195372\n",
      "Student: Test 7947/10000=0.794700, Train 467/500=0.934000\n",
      "Teacher: Test 7377/10000=0.737700, Train 500/500=1.000000\n",
      "Current Iter Num: 10000Time Spent: 12.360537\n",
      "Student: Test 8002/10000=0.800200, Train 468/500=0.936000\n",
      "Teacher: Test 7426/10000=0.742600, Train 499/500=0.998000\n",
      "Current Iter Num: 12000Time Spent: 12.937949\n",
      "Student: Test 7967/10000=0.796700, Train 465/500=0.930000\n",
      "Teacher: Test 7454/10000=0.745400, Train 500/500=1.000000\n",
      "Current Iter Num: 14000Time Spent: 13.084486\n",
      "Student: Test 7985/10000=0.798500, Train 464/500=0.928000\n",
      "Teacher: Test 7680/10000=0.768000, Train 499/500=0.998000\n",
      "Current Iter Num: 16000Time Spent: 13.464566\n",
      "Student: Test 7972/10000=0.797200, Train 464/500=0.928000\n",
      "Teacher: Test 7595/10000=0.759500, Train 500/500=1.000000\n",
      "Current Iter Num: 18000Time Spent: 13.066706\n",
      "Student: Test 8043/10000=0.804300, Train 471/500=0.942000\n",
      "Teacher: Test 7593/10000=0.759300, Train 500/500=1.000000\n",
      "Current Iter Num: 20000Time Spent: 13.315566\n",
      "Student: Test 7990/10000=0.799000, Train 465/500=0.930000\n",
      "Teacher: Test 7600/10000=0.760000, Train 500/500=1.000000\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(100)\n",
    "mx.random.seed(100)\n",
    "run_mnist_DistilledSGLD(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
